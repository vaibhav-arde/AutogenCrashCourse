{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c23826",
   "metadata": {},
   "source": [
    "# Selector Group Chat\n",
    "\n",
    "SelectorGroupChat is a group chat similar to RoundRobinGroupChat, but with a model-based next speaker selection mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22bae741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "# Model client\n",
    "model_client = OpenAIChatCompletionClient(model='gemini-1.5-flash-8b', api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97b28910",
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_agent = AssistantAgent(\n",
    "    name = 'PlanningAgent',\n",
    "    description= 'An agent for planning tasks,this agent should be the first to engage when given a new task.',\n",
    "    model_client=model_client,\n",
    "    system_message='''\n",
    "    You are a planning agent.\n",
    "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
    "    Your team members are :\n",
    "        WebSearchAgent : Searches for information.\n",
    "        DataAnalystAgent : Performs calculations\n",
    "\n",
    "    You only plan and delegate tasks - you do not exectue them yourself.\n",
    "\n",
    "    When assigning tasks, use the below format:\n",
    "    1. <agent> : <task>\n",
    "\n",
    "    After all the tasks are completed, summarize the findings and end with \"TERMINATE\"\n",
    "    ''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b992e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web_tool(query:str)-> str:\n",
    "    # Simulate a web search\n",
    "    if \"2006-2007\" in query:\n",
    "        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
    "        Udonis Haslem: 844 points\n",
    "        Dwayne Wade: 1397 points\n",
    "        James Posey: 550 points\n",
    "        ...\n",
    "        \"\"\"\n",
    "    elif \"2007-2008\" in query:\n",
    "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n",
    "    elif \"2008-2009\" in query:\n",
    "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n",
    "    return \"No data found.\"\n",
    "\n",
    "web_search_agent = AssistantAgent(\n",
    "    name = 'WebSearchAgent',\n",
    "    description= 'An agent for searching the web for information.',\n",
    "    model_client=model_client,\n",
    "    tools = [search_web_tool],\n",
    "    system_message='''\n",
    "        You are a web search agent.\n",
    "        Your only tool is search_web_tool - use it to find the information you need.\n",
    "\n",
    "        You make only one search call at a time.\n",
    "        \n",
    "        Once you have the results, you never do calculations or data analysis on them.\n",
    "    ''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81ed3905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_change_tool(start:float, end:float) -> float:\n",
    "    # Calculate percentage change\n",
    "    if start == 0:\n",
    "        return 0\n",
    "    return ((end - start) / start) * 100\n",
    "\n",
    "\n",
    "data_analyst_agent = AssistantAgent(\n",
    "    name = 'DataAnalystAgent',\n",
    "    description= 'An agent for performing calculations and data analysis.',\n",
    "    model_client=model_client,\n",
    "    tools= [percentage_change_tool],\n",
    "    system_message='''\n",
    "        You are a data analyst agent.\n",
    "        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n",
    "\n",
    "        If you have not seen the data, ask for it.\n",
    "\n",
    "    ''',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775373d",
   "metadata": {},
   "source": [
    "# Termination Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3edaf467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.conditions import MaxMessageTermination,TextMentionTermination\n",
    "\n",
    "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
    "max_messages_termination = MaxMessageTermination(max_messages=10)\n",
    "combined_termination = text_mention_termination | max_messages_termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd216016",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_prompt = '''\n",
    "Select an agent to perform the task.\n",
    "\n",
    "{roles}\n",
    "\n",
    "Current conversation history :\n",
    "{history}\n",
    "\n",
    "Read the above conversation, then select an agent from {participants} to perform the next task.\n",
    "Make sure the planning agent has assigned task before other agents start working.\n",
    "Only Select one agent.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a4ed145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTry not to overload the selector prompt with too much information.\\n\\n\\nselector_prompt (str, optional) – The prompt template to use for selecting the next speaker. \\n\\nAvailable fields: ‘{roles}’, ‘{participants}’, and ‘{history}’. \\n1. {participants} is the names of candidates for selection. The format is [“<name1>”, “<name2>”, …]. \\n2. {roles} is a newline-separated list of names and descriptions of the candidate agents. The format for each line is: “<name> : <description>”. \\n\\n3. {history} is the conversation history formatted as a double newline separated of names and message content. The format for each message is: “<name> : <message content>”.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Try not to overload the selector prompt with too much information.\n",
    "\n",
    "\n",
    "selector_prompt (str, optional) – The prompt template to use for selecting the next speaker. \n",
    "\n",
    "Available fields: ‘{roles}’, ‘{participants}’, and ‘{history}’. \n",
    "1. {participants} is the names of candidates for selection. The format is [“<name1>”, “<name2>”, …]. \n",
    "2. {roles} is a newline-separated list of names and descriptions of the candidate agents. The format for each line is: “<name> : <description>”. \n",
    "\n",
    "3. {history} is the conversation history formatted as a double newline separated of names and message content. The format for each message is: “<name> : <message content>”.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824b3be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "879ee173",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_team = SelectorGroupChat(\n",
    "    participants=[planning_agent, web_search_agent, data_analyst_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=combined_termination,\n",
    "    selector_prompt=selector_prompt,\n",
    "    allow_repeated_speaker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa7166b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Who was the Miami Heat player with the highest point in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e1ac23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Who was the Miami Heat player with the highest point in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\n",
      "---------- ToolCallRequestEvent (WebSearchAgent) ----------\n",
      "[FunctionCall(id='', arguments='{\"query\":\"Who was the Miami Heat player with the highest points in the 2006-2007 season?\"}', name='search_web_tool'), FunctionCall(id='', arguments='{\"query\":\"What was the percentage change in total rebounds for the Miami Heat player with the highest points in the 2006-2007 season between the 2007-2008 and 2008-2009 seasons?\"}', name='search_web_tool')]\n",
      "---------- ToolCallExecutionEvent (WebSearchAgent) ----------\n",
      "[FunctionExecutionResult(content='Here are the total points scored by Miami Heat players in the 2006-2007 season:\\n        Udonis Haslem: 844 points\\n        Dwayne Wade: 1397 points\\n        James Posey: 550 points\\n        ...\\n        ', name='search_web_tool', call_id='', is_error=False), FunctionExecutionResult(content='Here are the total points scored by Miami Heat players in the 2006-2007 season:\\n        Udonis Haslem: 844 points\\n        Dwayne Wade: 1397 points\\n        James Posey: 550 points\\n        ...\\n        ', name='search_web_tool', call_id='', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (WebSearchAgent) ----------\n",
      "Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
      "        Udonis Haslem: 844 points\n",
      "        Dwayne Wade: 1397 points\n",
      "        James Posey: 550 points\n",
      "        ...\n",
      "        \n",
      "Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
      "        Udonis Haslem: 844 points\n",
      "        Dwayne Wade: 1397 points\n",
      "        James Posey: 550 points\n",
      "        ...\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for WebSearchAgent_3292495b-d125-4d9d-aa83-13c5fd7ab73d/3292495b-d125-4d9d-aa83-13c5fd7ab73d\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "    ...<4 lines>...\n",
      "            await self._log_message(msg)\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 953, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "    ...<15 lines>...\n",
      "            yield inference_output\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1107, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_ext/models/openai/_openai_client.py\", line 691, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2583, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<48 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Please ensure that the number of function response parts is equal to the number of function call parts of the function call turn.', 'status': 'INVALID_ARGUMENT'}}]\n",
      "Error processing publish message for PlanningAgent_3292495b-d125-4d9d-aa83-13c5fd7ab73d/3292495b-d125-4d9d-aa83-13c5fd7ab73d\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 195, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for DataAnalystAgent_3292495b-d125-4d9d-aa83-13c5fd7ab73d/3292495b-d125-4d9d-aa83-13c5fd7ab73d\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 195, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Please ensure that the number of function response parts is equal to the number of function call parts of the function call turn.', 'status': 'INVALID_ARGUMENT'}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n    ...<4 lines>...\n            await self._log_message(msg)\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n    ...<15 lines>...\n            yield inference_output\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_ext/models/openai/_openai_client.py\", line 691, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2583, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<48 lines>...\n    )\n    ^\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1794, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1594, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Please ensure that the number of function response parts is equal to the number of function call parts of the function call turn.', 'status': 'INVALID_ARGUMENT'}}]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautogen_agentchat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mui\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Console\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(selector_team.run_stream(task=task))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:554\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    555\u001b[39m     stop_reason = message.message.content\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Please ensure that the number of function response parts is equal to the number of function call parts of the function call turn.', 'status': 'INVALID_ARGUMENT'}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n    ...<4 lines>...\n            await self._log_message(msg)\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n    ...<15 lines>...\n            yield inference_output\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/autogen_ext/models/openai/_openai_client.py\", line 691, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2583, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<48 lines>...\n    )\n    ^\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1794, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/vaibhavarde/Desktop/Autogen/AutogenCrashCourse/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1594, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Please ensure that the number of function response parts is equal to the number of function call parts of the function call turn.', 'status': 'INVALID_ARGUMENT'}}]\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "await Console(selector_team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee40df8",
   "metadata": {},
   "source": [
    "# Custom Selector Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0aeda1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"VaibhaV\"\n",
    "s[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "368e446e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PlanningAgent'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planning_agent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "167bb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\n",
    "\n",
    "def selector_func(messages : Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n",
    "    if(messages[-1].source != planning_agent.name):\n",
    "        return planning_agent.name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "629486bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "await selector_team.reset()\n",
    "selector_team = SelectorGroupChat(\n",
    "    participants=[planning_agent, web_search_agent, data_analyst_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=combined_termination,\n",
    "    selector_prompt=selector_prompt,\n",
    "    allow_repeated_speaker=True,\n",
    "    selector_func=selector_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b87ecb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Who was the Miami Heat player with the highest point in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\n",
      "---------- TextMessage (PlanningAgent) ----------\n",
      "1. WebSearchAgent : Find the Miami Heat player with the highest points scored in the 2006-2007 season.\n",
      "\n",
      "2. WebSearchAgent : Find the total rebounds for the Miami Heat player with the highest points in the 2006-2007 season for the 2007-2008 season.\n",
      "\n",
      "3. WebSearchAgent : Find the total rebounds for the same player for the 2008-2009 season.\n",
      "\n",
      "4. DataAnalystAgent : Calculate the percentage change in total rebounds between the 2007-2008 and 2008-2009 seasons for the player identified in step 2.\n",
      "\n",
      "5. WebSearchAgent : Verify the data accuracy for the calculations performed in step 4.\n",
      "\n",
      "Summarize the findings from the data analysis and the web searches, and then TERMINATE.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(id='c66ba49b-d952-490e-b4e2-0b52b00ed7b7', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 27, 13, 21, 11, 844658, tzinfo=datetime.timezone.utc), content='Who was the Miami Heat player with the highest point in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?', type='TextMessage'), TextMessage(id='0099eb0b-e777-40d6-bd14-a78465ce8649', source='PlanningAgent', models_usage=RequestUsage(prompt_tokens=168, completion_tokens=193), metadata={}, created_at=datetime.datetime(2025, 8, 27, 13, 21, 13, 341463, tzinfo=datetime.timezone.utc), content='1. WebSearchAgent : Find the Miami Heat player with the highest points scored in the 2006-2007 season.\\n\\n2. WebSearchAgent : Find the total rebounds for the Miami Heat player with the highest points in the 2006-2007 season for the 2007-2008 season.\\n\\n3. WebSearchAgent : Find the total rebounds for the same player for the 2008-2009 season.\\n\\n4. DataAnalystAgent : Calculate the percentage change in total rebounds between the 2007-2008 and 2008-2009 seasons for the player identified in step 2.\\n\\n5. WebSearchAgent : Verify the data accuracy for the calculations performed in step 4.\\n\\nSummarize the findings from the data analysis and the web searches, and then TERMINATE.\\n', type='TextMessage')], stop_reason=\"Text 'TERMINATE' mentioned\")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "await Console(selector_team.run_stream(task=task))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogencrashcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
